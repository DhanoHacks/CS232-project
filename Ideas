Reference Paper 1 : Exploring Core and Cache Hierarchy Bottlenecks in Graph Processing Workloads
            Abanti Basak , Xing Hu, Shuangchen Li, Sang Min Oh , and Yuan Xie

1) The private L2 cache shows negligible impact on improving system performance 
2) the shared L3 cache shows higher performance sensitivity
3) the data type representing the graph property benefits the most from a larger shared L3 cache
4) any cacheline in the recency stack other than the most recently used (MRU) cacheline is a viable candidate for eviction 
5) The L2 hit rate and the speedup show little sensitivity to the different L2 configurations (both capacity and set associativity).
    Therefore, an architecture without private L2 caches is just as fine for graph processing.

In 1) issue is that we don't know the relation between access latencies and cache sizes -- trying to figure out an approximation!

4) - Implemented and results match. MRU performs worst
5) Bypassing L2 cache
    Tried reducing size of L2 and latency to 0 -- did not work as we still had a level between L1 and LLC.







Replacement policies implemented and plotted:

LRU
LFU
FIFO
RANDOM
MRU
SRRIP


About SSRIP: 
High Performance Cache Replacement Using Re-Reference Interval Prediction (RRIP)
Aamer Jaleel † Kevin B. Theobald ‡ Simon C. Steely Jr. † Joel Emer †

Fig 3 explains the working of SRRIP
1) We propose cache replacement using Re-reference Interval Prediction (RRIP). RRIP statically predicts the re-reference
interval of all missing cache blocks to be an intermediate re-reference interval that is between a near-immediate re-reference
interval and a distant re-reference interval. RRIP updates the re-reference prediction to be shorter than the previous prediction
upon a re-reference. We call this policy as Static RRIP (SRRIP). We show that SRRIP is scan-resistant and only requires 2-bits per cache block.

2) We propose two SRRIP policies: SRRIP-Hit Priority (SRRIP-HP) and SRRIP-Frequency Priority (SRRIP-FP). SRRIP-HP predicts that any cache block that receives a hit will have a near-immediate re-reference and thus should be retained in the cache for an extended period of time. SRRIP-FP on the other hand predicts that frequently referenced cache blocks will have a near-immediate re-reference and thus they should be retained in the cache for an extended period of time. We show that SRRIP-HP performs significantly better than SRRIP-FP and conclude that scan-resistance is not from precisely detecting
frequently referenced blocks but from preventing blocks that receive hits from getting evicted by blocks that do not receive hits (i.e., scan blocks).


IMP -- two policies to update the re-reference prediction: Hit Priority (HP) and Frequency Priority (FP). The RRIP-HP policy
predicts that the block receiving a hit will be re-referenced in the near-immediate future and updates the RRPV of the associated block
to zero. The goal of the HP policy is to prioritize replacement of blocks that do not receive cache hits over any cache block that receives a hit. However, the HP policy can potentially degrade cache performance when a cache block is re-referenced only once after cache insertion. In such situations, the HP policy incorrectly predicts a near-immediate re-reference prediction instead of distant re-reference prediction for the block and causes the block to occupy valuable cache space without receiving any hits. To address this problem, the RRIP-FP policy uses more information (i.e., cache hits)
to update the re-reference prediction. Instead of updating the re-reference prediction to be near-immediate on a hit, RRIP-FP updates
the predicted re-reference interval to be shorter than the previous re-reference interval each time a block receives a hit. The FP policy
accomplishes this by decrementing the RRPV register (unless the RRPV register is already zero) on cache hits. The goal of the FP policy is to prioritize replacement of infrequently re-referenced cache blocks over frequently re-referenced cache blocks.

Other papers read which provide similar insight as Paper 1 but use software solutions in their core -- therefore we can't implement those
GRASP
GRACE
CAGRA


ChampSim by default uses NON-INCLUSIVE cache hierarchy.
We implement :
1) INCLUSIVE
2) EXCLUSIVE


